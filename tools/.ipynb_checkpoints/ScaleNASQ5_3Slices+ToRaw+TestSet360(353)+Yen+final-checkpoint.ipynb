{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from math import ceil, floor\n",
    "import cv2\n",
    "import sys\n",
    "import json\n",
    "# from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CT_EXCEL_FILE = '/mnt/fast-disk1/mjc/AutoRecist/Codes/ScaleNASDiceLoss/tools/AutoRECIST_Evaluation_Image_And_Contour_20220826.xlsx'\n",
    "\n",
    "\n",
    "# xls = pd.ExcelFile(CT_EXCEL_FILE)\n",
    "\n",
    "# # Now you can list all sheets in the file\n",
    "# print( xls.sheet_names )\n",
    "# # ['house', 'house_extra', ...]\n",
    "\n",
    "# # to read just one sheet to dataframe:\n",
    "# df_all = pd.read_excel(xls, sheet_name=\"Sheet2\")\n",
    "\n",
    "\n",
    "# df = df_all # a liver lesions case\n",
    "\n",
    "# df = df.drop_duplicates(subset=['Image File Path'], keep='first')\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def window_image(img, window_center,window_width, intercept, slope):\n",
    "    \n",
    "#     window_center,window_width = 50 ,100\n",
    "    img = (img*slope +intercept)\n",
    "    img_min = window_center - window_width//2\n",
    "    img_max = window_center + window_width//2\n",
    "    img[img<img_min] = img_min\n",
    "    img[img>img_max] = img_max\n",
    "    return img \n",
    "\n",
    "\n",
    "def get_first_of_dicom_field_as_int(x):\n",
    "    #get x[0] as in int is x is a 'pydicom.multival.MultiValue', otherwise get int(x)\n",
    "    if type(x) == pydicom.multival.MultiValue:\n",
    "        return int(x[0])\n",
    "    else:\n",
    "        return int(x)\n",
    "\n",
    "def get_windowing(data):\n",
    "    dicom_fields = [data[('0028','1050')].value, #window center\n",
    "                    data[('0028','1051')].value, #window width\n",
    "                    data[('0028','1052')].value, #intercept\n",
    "                    data[('0028','1053')].value] #slope\n",
    "    return [get_first_of_dicom_field_as_int(x) for x in dicom_fields]\n",
    "\n",
    "def _normalize(img):\n",
    "    if img.max() == img.min():\n",
    "        return np.zeros(img.shape)-1\n",
    "    return 2 * (img - img.min())/(img.max() - img.min()) - 1\n",
    "\n",
    "def normalize_minmax(img):\n",
    "    mi, ma = img.min(), img.max()\n",
    "    if mi == ma:\n",
    "        return np.zeros(img.shape)-1\n",
    "    return 2*(img - mi) / (ma - mi) - 1\n",
    "\n",
    "def getName(s):\n",
    "    ix1 = s.rfind('/')\n",
    "    ix2 = s.rfind('.')\n",
    "    return s[ix1:ix2]\n",
    "\n",
    "\n",
    "def _read(path, desired_size = (512,512)):\n",
    "    \"\"\"Will be used in DataGenerator\"\"\"\n",
    "\n",
    "    try:\n",
    "        data = pydicom.read_file(path)\n",
    "        image = data.pixel_array\n",
    "        window_center , window_width, intercept, slope = get_windowing(data)\n",
    "        \n",
    "        image_windowed = window_image(image, window_center, window_width, intercept, slope)\n",
    "        img = normalize_minmax(image_windowed)\n",
    "\n",
    "    except:\n",
    "        img = np.zeros(desired_size[:2])-1\n",
    "    \n",
    "    if img.shape[:2] != desired_size[:2]:\n",
    "        print(\"image shape is not desired size. Interpolation is done.\")\n",
    "        img = cv2.resize(img, desired_size[:2], interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "D_dir2header_df = {}\n",
    "\n",
    "\n",
    "def get_dicom_header_df(image_dir , labels = []):\n",
    "    global D_dir2header_df\n",
    "    if image_dir in D_dir2header_df:\n",
    "        return D_dir2header_df[image_dir]\n",
    "\n",
    "    # image_dir = row['Image File Path']\n",
    "\n",
    "\n",
    "    labels = ['ImageName','InstanceNumber',\n",
    "            'BitsAllocated', 'BitsStored', 'Columns', 'HighBit', \n",
    "            'ImageOrientationPatient_0', 'ImageOrientationPatient_1', 'ImageOrientationPatient_2',\n",
    "            'ImageOrientationPatient_3', 'ImageOrientationPatient_4', 'ImageOrientationPatient_5',\n",
    "            'ImagePositionPatient_0', 'ImagePositionPatient_1', 'ImagePositionPatient_2',\n",
    "            'Modality', 'PatientID', 'PhotometricInterpretation', 'PixelRepresentation',\n",
    "            'PixelSpacing_0', 'PixelSpacing_1', 'RescaleIntercept', 'RescaleSlope', 'Rows', 'SOPInstanceUID',\n",
    "            'SamplesPerPixel', 'SeriesInstanceUID', 'StudyID', 'StudyInstanceUID', \n",
    "            'WindowCenter', 'WindowWidth', \n",
    "        ] if not labels else labels\n",
    "\n",
    "    data = {l: [] for l in labels}\n",
    "    \n",
    "    ctList = os.listdir(image_dir)\n",
    "    ctList.sort()\n",
    "\n",
    "    for image in ctList:\n",
    "        if '.dcm' not in image:\n",
    "            continue\n",
    "        if os.path.getsize(os.path.join(image_dir, image)) < 5*1024:\n",
    "            print('%s size < 5kb skiped!'%os.path.join(image_dir, image) )\n",
    "            continue\n",
    "        data[\"ImageName\"].append(image)\n",
    "\n",
    "        ds = pydicom.dcmread(os.path.join(image_dir, image))\n",
    "        for metadata in ds.dir():\n",
    "            if metadata not in data and metadata not in ['ImageOrientationPatient','ImagePositionPatient','PixelSpacing']:\n",
    "                continue\n",
    "            if metadata != \"PixelData\":\n",
    "                metadata_values = getattr(ds, metadata)\n",
    "                if type(metadata_values) == pydicom.multival.MultiValue and metadata not in [\"WindowCenter\", \"WindowWidth\"]:\n",
    "                    for i, v in enumerate(metadata_values):\n",
    "                        data[f\"{metadata}_{i}\"].append(v)  \n",
    "                else:\n",
    "                    if type(metadata_values) == pydicom.multival.MultiValue and metadata in [\"WindowCenter\", \"WindowWidth\"]:\n",
    "                        data[metadata].append(metadata_values[0])\n",
    "                    else:\n",
    "                        data[metadata].append(metadata_values)\n",
    "\n",
    "    df_image = pd.DataFrame(data).set_index(\"InstanceNumber\")\n",
    "    D_dir2header_df[image_dir] = df_image\n",
    "    return df_image\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "def InstanceNumber2file_name(df_image, num):\n",
    "    return df_image.loc[num,'ImageName']\n",
    "\n",
    "def InstanceNumber2data_element(df_image, num, label):\n",
    "    return df_image.loc[num , label]\n",
    "\n",
    "    \n",
    "def get_SliceThickness(df_image):\n",
    "    flag = False\n",
    "    L = df_image['ImagePositionPatient_2'].tolist()\n",
    "    thick = list( np.diff(L) )\n",
    "    res = float( max(set(thick), key=thick.count) )\n",
    "    res = -res if res < 0 else res\n",
    "    \n",
    "    L.sort()\n",
    "    thick2 = list( np.diff(L) )\n",
    "    res2 = float( max(set(thick2), key=thick2.count) )\n",
    "    if res2 ==0 and res==0:\n",
    "        result = 0\n",
    "        flag = True\n",
    "        print('Warning intv is 0')\n",
    "        print(df_image['ImagePositionPatient_2'])\n",
    "    if res2 == res:\n",
    "        result = res\n",
    "    else:\n",
    "        result = res\n",
    "        flag = True\n",
    "        print('Warning intv may wrong',res,res2)\n",
    "        print(df_image['ImagePositionPatient_2'])\n",
    "    \n",
    "    return result \n",
    "\n",
    "def InstanceNumber2windows_min_max(df_image,num):\n",
    "    try:     \n",
    "        WL = InstanceNumber2data_element(df_image, num, 'WindowCenter')\n",
    "        WW = InstanceNumber2data_element(df_image, num, 'WindowWidth')\n",
    "    except:\n",
    "        print(\"Warning! Window Center or Width is empty! Now use default values\")\n",
    "        WL , WW = 250 , 1500\n",
    "        \n",
    "    minHU = int( WL-WW/2 )\n",
    "    maxHU = minHU + int(WW)\n",
    "    return [minHU , maxHU]\n",
    "\n",
    "\n",
    "class ASerial:\n",
    "    P=-1\n",
    "    D=-1\n",
    "    S=-1\n",
    "    name = ''\n",
    "    def __init__(self, path_str):\n",
    "        self.path = path_str\n",
    "        self.getP()\n",
    "        self.getD()\n",
    "        self.getS()\n",
    "        self.convert_path()\n",
    "        \n",
    "    def getP(self, target = 'DeepLesion_', L=6):\n",
    "        ix = self.path.rfind(target) + len(target)\n",
    "        ss = self.path[ix:ix+L]\n",
    "        self.P = int(ss)\n",
    "        \n",
    "    def getD(self, target = '/D', L=6):\n",
    "        ix = self.path.rfind(target) + len(target)\n",
    "        ss = self.path[ix:ix+L]\n",
    "        self.D = int(ss)\n",
    "        \n",
    "    def getS(self, target = '/S', L=6):\n",
    "        ix = self.path.rfind(target) + len(target)\n",
    "        ss = self.path[ix:ix+L]\n",
    "        self.S = int(ss)\n",
    "        \n",
    "    def convert_path(self):\n",
    "        self.name = '%06d_%02d_%02d'%(self.P, self.D, self.S)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def replacer(s, newstring, index, nofail=False):\n",
    "    # raise an error if index is outside of the string\n",
    "    if not nofail and index not in range(len(s)):\n",
    "        raise ValueError(\"index outside given string\")\n",
    "\n",
    "    # if not erroring, but the index is still not in the correct range..\n",
    "    if index < 0:  # add it to the beginning\n",
    "        return newstring + s\n",
    "    if index > len(s):  # add it to the end\n",
    "        return s + newstring\n",
    "\n",
    "    # insert the new string between \"slices\" of the original\n",
    "    return s[:index] + newstring + s[index + 1:]\n",
    "\n",
    "def convert_file_name(name,S='/'):\n",
    "    ix = name.rfind('_')\n",
    "    return replacer(name,S,ix)\n",
    "\n",
    "def file_name2id(name):\n",
    "    name.replace('.png','')\n",
    "    name.replace('_','')\n",
    "    return int('1' + name)\n",
    "    \n",
    "def get_image_size( s ):\n",
    "    num = list( map( int , s.split(',')))\n",
    "    return num[0] , num[1]\n",
    "\n",
    "def get_spacing( s ):\n",
    "    num = list( map( float , s.split(',')))\n",
    "    return num[0] , num[1] , num[2]\n",
    "\n",
    "\n",
    "def get_z_position( df ):\n",
    "    s = df.loc['Normalized_lesion_location']\n",
    "    num = list( map( float , s.split(',')))\n",
    "    return num[2]\n",
    "    \n",
    "def get_slice_no( df ):\n",
    "    s = df.loc['Key_slice_index']\n",
    "    return int(s)\n",
    "\n",
    "def get_windows( df ):\n",
    "    s = df.loc[ 'DICOM_windows']\n",
    "    num = list( map( float , s.split(',')))\n",
    "    return num\n",
    "\n",
    "\n",
    "def get_segmentation():\n",
    "    return []\n",
    "\n",
    "def get_bbox( df ):\n",
    "    s = df.loc['Bounding_boxes']\n",
    "    num = list( map( float , s.split(',')))\n",
    "    num[2] = num[2]-num[0]\n",
    "    num[3] = num[3]-num[1]\n",
    "    return num \n",
    "\n",
    "def get_noise( df ):\n",
    "    s = df.loc['Possibly_noisy']\n",
    "    num = int(s)\n",
    "    return num\n",
    "\n",
    "def get_area( df ):\n",
    "    s = df.loc['Lesion_diameters_Pixel_']\n",
    "    num = list( map( float , s.split(',')))\n",
    "    return num[0]*num[1]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "newcats = [{'supercategory': 'DeepLesion', 'id': 1, 'name': 'abdomen'},\n",
    "           {'supercategory': 'DeepLN', 'id': 2, 'name': 'abdomen LN'},\n",
    "           {'supercategory': 'DeepLesion', 'id': 3, 'name': 'adrenal'},\n",
    "           {'supercategory': 'DeepLN', 'id': 4, 'name': 'axillary LN'},\n",
    "           {'supercategory': 'DeepLesion', 'id': 5, 'name': 'bone'},\n",
    "           {'supercategory': 'DeepLN', 'id': 6, 'name': 'inguinal LN'},\n",
    "           {'supercategory': 'DeepLesion', 'id': 7, 'name': 'kidney'},\n",
    "           {'supercategory': 'DeepLesion', 'id': 8, 'name': 'liver'},\n",
    "           {'supercategory': 'DeepLesion', 'id': 9, 'name': 'lung'},\n",
    "           {'supercategory': 'DeepLN', 'id': 10, 'name': 'mediastinum LN'},\n",
    "           {'supercategory': 'DeepLN', 'id': 11, 'name': 'neck LN'},\n",
    "           {'supercategory': 'DeepLesion', 'id': 12, 'name': 'ovary'},\n",
    "           {'supercategory': 'DeepLesion', 'id': 13, 'name': 'pancreas'},\n",
    "           {'supercategory': 'DeepLN', 'id': 14, 'name': 'pelvic LN'},\n",
    "           {'supercategory': 'DeepLesion', 'id': 15, 'name': 'pelvis'},\n",
    "           {'supercategory': 'DeepLesion', 'id': 16, 'name': 'pleural'},\n",
    "           {'supercategory': 'DeepLN', 'id': 17, 'name': 'retroperitoneal LN'},\n",
    "           {'supercategory': 'DeepLesion', 'id': 18, 'name': 'soft tissue'},\n",
    "           {'supercategory': 'DeepLesion', 'id': 19, 'name': 'spleen'},\n",
    "           {'supercategory': 'DeepLesion', 'id': 20, 'name': 'stomach'},\n",
    "           {'supercategory': 'DeepLesion', 'id': 21, 'name': 'thyroid'} ]\n",
    "\n",
    "def get_21_lesion_location_cls():\n",
    "    D_cls = {}\n",
    "    for d in newcats:\n",
    "        id_ = d['id']\n",
    "        name = d['name']\n",
    "        D_cls[name] = id_\n",
    "    return D_cls\n",
    "\n",
    "D_cls = get_21_lesion_location_cls()\n",
    "\n",
    "def get_category_id( location , Dict ):\n",
    "    return Dict[location]\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "def replace_png_path(s):\n",
    "    cs = s.replace('AutoRecist/Inputs' , 'AutoRecist/Pngs')\n",
    "    return cs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DeepLesion():\n",
    "    \"\"\"\n",
    "        DL class to convert annotations to COCO Json format\n",
    "    \"\"\"\n",
    "    def __init__(self, df,image_id_start=0,annotation_id_start=0, savename='a.json'):\n",
    "        self.image_id_start = image_id_start\n",
    "        self.annotation_id_start = annotation_id_start\n",
    "        self.df = df \n",
    "        self.info = {\"year\" : 2021,\n",
    "                     \"version\" : \"2.0\",\n",
    "                     \"description\" : \"Covert Weasis to Json format\",\n",
    "                     \"contributor\" : \"HY,JM,BZ,LS,FSA\",\n",
    "                     \"url\" : \"http:// /\",\n",
    "                     \"date_created\" : \"20211129\"\n",
    "                    }\n",
    "        self.licenses = [{\"id\": 1,\n",
    "                          \"name\": \"Attribution-NonCommercial\",\n",
    "                          \"url\": \"http:// /\"\n",
    "                         }]\n",
    "\n",
    "        self.categories = newcats\n",
    "        \n",
    "        self.images, self.annotations = self.__get_image_annotation_pairs__(self.df)\n",
    "        json_data = {\"info\" : self.info,\n",
    "                     \"images\" : self.images,\n",
    "                     \"licenses\" : self.licenses,\n",
    "                     \"annotations\" : self.annotations,\n",
    "                     \"categories\" : self.categories}\n",
    "\n",
    "        with open(savename, \"w\") as jsonfile:\n",
    "            json.dump(json_data, jsonfile, sort_keys=True, indent=4)\n",
    "            \n",
    "    def change_df(self , df , savename = 'temp.json'):\n",
    "        self.df = df \n",
    "\n",
    "        self.images, self.annotations = self.__get_image_annotation_pairs__(self.df)\n",
    "        json_data = {\"info\" : self.info,\n",
    "                     \"images\" : self.images,\n",
    "                     \"licenses\" : self.licenses,\n",
    "                     \"annotations\" : self.annotations,\n",
    "                     \"categories\" : self.categories}\n",
    "\n",
    "        with open(savename, \"w\") as jsonfile:\n",
    "            json.dump(json_data, jsonfile, sort_keys=True, indent=4)\n",
    "            print( 'Saved %s'%savename )\n",
    "        \n",
    "            \n",
    "    def __get_image_annotation_pairs__(self,df):\n",
    "        images = []\n",
    "        annotations = []\n",
    "        self.file_name_dict = {}\n",
    "        for i , row in df.iterrows():\n",
    "            try:\n",
    "                print(i)\n",
    "                df_image = get_dicom_header_df( row['Image File Path'] )\n",
    "                png_folder = replace_png_path(row['Image File Path'] )\n",
    "                \n",
    "                for one in df_image.index.values.tolist():\n",
    "#                     file_name = InstanceNumber2file_name(df_image, one)\n",
    "#                     file_name = os.path.join( row['Image File Path'] , file_name)\n",
    "                    file_name = os.path.join(png_folder, '%03d.png'%one)\n",
    "                    file_name = file_name.replace('/mnt/fast-disk1/mjc/AutoRecist/','')\n",
    "\n",
    "                    if file_name in self.file_name_dict:\n",
    "                        oneimageid = self.file_name_dict[file_name]\n",
    "                    else:\n",
    "                        oneimage = {}\n",
    "                        oneimage['file_name'] = file_name\n",
    "                        self.image_id_start += 1\n",
    "                        oneimageid = self.image_id_start\n",
    "                        oneimage['id'] = oneimageid\n",
    "\n",
    "                        oneimage['height'] , oneimage['width'] = int(InstanceNumber2data_element(df_image,one,'Rows')), int( InstanceNumber2data_element(df_image,one,'Columns') )\n",
    "\n",
    "                        oneimage['slice_no'] = int(one)\n",
    "                        oneimage['spacing'] = float( InstanceNumber2data_element(df_image,one,'PixelSpacing_0') )\n",
    "                        oneimage['slice_intv'] = float( get_SliceThickness(df_image) )\n",
    "                        oneimage['z_position'] = 0.5\n",
    "                        oneimage['windows'] = InstanceNumber2windows_min_max(df_image,one)\n",
    "\n",
    "                        images.append(oneimage)\n",
    "                        self.file_name_dict[file_name] = oneimageid\n",
    "\n",
    "\n",
    "            except Exception as e: print(e)\n",
    "        \n",
    "        return images, annotations\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "experiment_name = 'Lesion_Q5_scalenet_seg_test'\n",
    "mask_name = 'mask_1514'\n",
    "arglist = ['--cfg', '../experiments/lesion_Q5/%s.yaml'%experiment_name ,  \n",
    "           '--mask_path', '../evo_files/masks/%s.npy'%mask_name,  \n",
    "           'TEST.MODEL_FILE', '../output/Lesion/superscalenet_seg/Lesion_Q5_superscalenet_base/data_patch_train/best.pth',\n",
    "           'DATASET.ROOT','',\n",
    "           'TRAIN.USE_FLIPPED',False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This notebook is to load predicted mask for hard-disk and calculate Segmentation evaluation metics.\n",
    "# predicted mask is the output of ScaleNASv2 Test and save predicted mask.ipynb\n",
    "# \n",
    "# cp -v /mnt/fast-data/mjc/AutoRECIST/Codes/ScaleNAS/ScaleNASv1/tools/utils_test.py .\n",
    "# \n",
    "# gt box are loaded from /cache/*gt_roidb.pkl\n",
    "# gt segmentation are loaded from Hao's Raw files\n",
    "# \n",
    "# predictions are all_boxes and all_segms which both are loaded from mask_1988 png files.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # This file is for segmetation metrics evaluation in 3D\n",
    "# Edited by Jingchen around 06/20/2021\n",
    "# This file is after ScaleNAS test which save predition into png images.\n",
    "# This file load png images as predicted contours in 2D\n",
    "# load cache pkl as gold-standard contours in 2D\n",
    "# The stack 2D based on dicom-header to get 3D\n",
    "# Evaluate 3D metics of dice, IoU, over-segmetation, and under-segmetation\n",
    "# \n",
    "get_ipython().magic('reload_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import _init_paths\n",
    "\n",
    "from config import cfg\n",
    "from config import update_config\n",
    "sys.path.append('/mnt/fast-disk1/mjc/utils_codes/')\n",
    "from utils_test import *\n",
    "from utils_test import __get_annotation__\n",
    "from utils_metrics_3d import *\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "sys.path.append('/mnt/fast-disk1/mjc/utils_codes/read_weasis_raw_v0.96/')\n",
    "import weasis_raw_data_api as wr\n",
    "\n",
    "def convert_name(name):\n",
    "    new = name.replace('/','_')\n",
    "    return new\n",
    "\n",
    "HEIGHT , WIDTH = 512, 512\n",
    "def get_pred_vol(oneCT , site_list , D_z_index, union_mask = True):\n",
    "    slice_no_list =list ( oneCT.keys() )\n",
    "\n",
    "    V = D_z_index.values()\n",
    "    shape_z = np.max(list(V)) + 1\n",
    "    vol_shape = (shape_z , HEIGHT , WIDTH )\n",
    "    height = vol_shape[1]\n",
    "    width = vol_shape[2]\n",
    "\n",
    "    if len(slice_no_list):\n",
    "        slice_no_list.sort()\n",
    "        vol_gt = np.zeros(vol_shape, dtype = bool)\n",
    "        vol_pred = np.zeros(vol_shape, dtype = bool)\n",
    "\n",
    "        for s in slice_no_list:\n",
    "            aroidb , bboxes , segmentations = oneCT[s]\n",
    "\n",
    "            ix = [a for a,b in enumerate(aroidb['gt_classes']) if int(b) in site_list]\n",
    "            contours = [ aroidb['segms'][int(kk)] for kk in ix ]\n",
    "            \n",
    "            for c in contours:\n",
    "                if len(c): #gt\n",
    "                    new = polys_to_mask(c , height , width)\n",
    "                    vol_gt[D_z_index[s]][new>0] = 1 \n",
    "\n",
    "\n",
    "            for j in site_list:\n",
    "                contours = segmentations[j]\n",
    "                if union_mask:\n",
    "                    #contour should be numpy.array here. list cause error of no attribute 'flatten'\n",
    "                    cc = [ contour.flatten().tolist() for contour in contours if len(contour)!=0]\n",
    "                    contours = union_ploys(cc , height, width)\n",
    "\n",
    "                for c in contours:#union pred\n",
    "                    if len(c)>=6:\n",
    "                        new = polys_to_mask([c] , height , width) \n",
    "                        vol_pred[D_z_index[s]][new>0] = 1 \n",
    "                    elif len(c):\n",
    "                        print('len pred contour is %d'%len(c))\n",
    "    return vol_pred\n",
    "\n",
    "def seperate_vol(vol_pred , reduceFP = False):\n",
    "    # vol_dict = seperate_vol(vol_pred)\n",
    "    connectivity = 2\n",
    "    from skimage import measure\n",
    "    labels_pred=measure.label(vol_pred,connectivity=connectivity)\n",
    "    l_pred,c_pred = np.unique(labels_pred , return_counts=True)\n",
    "\n",
    "\n",
    "    ix2 = l_pred>0\n",
    "    l_pred = l_pred[ix2] #background pixels are labeled as 0, so we exclude them\n",
    "    c_pred = c_pred[ix2]\n",
    "\n",
    "    if reduceFP:\n",
    "        ix2 = l_pred>0\n",
    "        for i, p in enumerate(l_pred):\n",
    "            z = np.where(labels_pred == p)[0]\n",
    "            if len( set(z) )<=1:\n",
    "                ix2[i]=False\n",
    "\n",
    "        l_pred = l_pred[ix2] #background pixels are labeled as 0, so we exclude them\n",
    "        c_pred = c_pred[ix2]\n",
    "\n",
    "\n",
    "    vol_dict = {}\n",
    "\n",
    "    for p in l_pred:\n",
    "        vp = labels_pred == p\n",
    "        vol_dict[p] = vp\n",
    "\n",
    "    return vol_dict\n",
    "\n",
    "\n",
    "\n",
    "site_list_liver = [8]\n",
    "site_list_liver_lung_LNs = [2,4,6,8,9,10,11,14,17] \n",
    "site_list_LNs = [2,4,6,10,11,14,17] \n",
    "\n",
    "site_list = site_list_liver\n",
    "user_id = 'jm4669'\n",
    "\n",
    "cache_path = '/mnt/fast-disk1/mjc/AutoRecist/Codes/ScaleNASDiceLoss/tools/cache/'\n",
    "name = 'inference'\n",
    "# name = 'lesion_train'\n",
    "\n",
    "cache_filepath = os.path.join(cache_path, name+'_gt_roidb.pkl')\n",
    "# print('Loading cached gt_roidb from %s', cache_filepath)\n",
    "with open(cache_filepath, 'rb') as fp:\n",
    "    cached_roidb = pickle.load(fp)\n",
    "    \n",
    "roidb = cached_roidb\n",
    "\n",
    "\n",
    "\n",
    "sv_dir = mask_name\n",
    "sv_path = os.path.join(sv_dir, 'test_val_results')\n",
    "print( 'load AI outputs from {}'.format(sv_path) )\n",
    "\n",
    "all_boxes = [ [ np.zeros((0,5),dtype=\"float32\") for _ in range(len(roidb)) ] for _ in range( cfg.DATASET.NUM_CLASSES) ]\n",
    "all_segms = [ [ [] for _ in range(len(roidb)) ] for _ in range( cfg.DATASET.NUM_CLASSES) ]\n",
    "\n",
    "for i in range(len(roidb)):\n",
    "\n",
    "    one = roidb[i]\n",
    "    onename = one['image']\n",
    "    if not os.path.exists( os.path.join( sv_path, convert_name(onename) ) ):\n",
    "        print(os.path.join(sv_path, convert_name(onename) ) , 'not exists!')\n",
    "    pred_im = Image.open(os.path.join( sv_path, convert_name(onename) ))\n",
    "    pred = np.array(pred_im)\n",
    "    for j in range(cfg.DATASET.NUM_CLASSES):\n",
    "        mask = np.asarray( pred==j , dtype=np.uint8)\n",
    "        if np.sum(mask > 0) <= 3 :\n",
    "            continue\n",
    "        segmentation, bbox, area = __get_annotation__(mask , xywh = False , bbox_score=True)\n",
    "        if segmentation and bbox:\n",
    "            all_segms[j][i] = segmentation\n",
    "            all_boxes[j][i] = bbox\n",
    "\n",
    "\n",
    "D_CT = {}\n",
    "for i , aroidb in enumerate(roidb):\n",
    "    dicom_path , png_name = os.path.split(aroidb['image'])\n",
    "    slice_no , _= os.path.splitext(png_name)\n",
    "    slice_no = int(slice_no)\n",
    "    if slice_no != aroidb['slice_no']:\n",
    "        print('following slice numbers are not consistence.')\n",
    "        print(dicom_path,slice_no,aroidb['slice_no'])\n",
    "\n",
    "    segmentations = {}\n",
    "    bboxes = {}\n",
    "    for j in site_list:\n",
    "        segmentations[j] = all_segms[j][i]\n",
    "        bboxes[j] = all_boxes[j][i]\n",
    "\n",
    "    if dicom_path not in D_CT:\n",
    "        D_CT[dicom_path] = {}\n",
    "        D_CT[dicom_path][slice_no] = [aroidb , bboxes , segmentations]\n",
    "    else:\n",
    "        D_CT[dicom_path][slice_no] = [aroidb , bboxes , segmentations]\n",
    "\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "def initialize_mask_vol( D_z_index , height , width):\n",
    "    V = D_z_index.values()\n",
    "    shape_z = np.max(list(V)) + 1\n",
    "    mask_vol = np.zeros((shape_z , height , width ) , dtype=np.uint8 )\n",
    "    return mask_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = '/mnt/fast-disk1/refine_gt/'\n",
    "SAVE_FOLDER_NAME = 'ScaleNASQ5_3Slices_ToRaw_TestYenFinal360'\n",
    "Metrics_vol = []\n",
    "keys = list(D_CT.keys())\n",
    "for k in keys:\n",
    "#     if 'COU-AA-302' in k:\n",
    "#         site_list = site_list_LNs\n",
    "#     else:\n",
    "#         site_list = site_list_liver_lung_LNs\n",
    "\n",
    "    image_series_path = k.replace('/Pngs/' , '/Inputs/')    \n",
    "\n",
    "    df_image = get_dicom_header_df( image_series_path )\n",
    "    instanceNumber_list = df_image.index.to_list()\n",
    "    D_z_index = instanceNumber2Matrix_z_index(instanceNumber_list)\n",
    "\n",
    "\n",
    "    oneCT = remove_single_slice_segms(D_CT[k])\n",
    "    vol_pred = get_pred_vol(oneCT , site_list , D_z_index, union_mask = False)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    image_series = wr.dicom_header(image_series_path)\n",
    "    if len(image_series):\n",
    "        height = image_series[0].Rows \n",
    "        width = image_series[0].Columns \n",
    "    else:\n",
    "        print('ERROR image_series has no len' , image_series_path)\n",
    "    assert(len(image_series) == vol_pred.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "    vol2 = vol_pred\n",
    "    vol2[vol2>1]=1\n",
    "    vol_dict = seperate_vol(vol2)\n",
    "    \n",
    "    for tumor_index in vol_dict:\n",
    "        mask_volume = vol_dict[tumor_index]\n",
    "        weasis_raw_data = wr.create(image_series, mask_volume)\n",
    "\n",
    "        file_folder = os.path.join(SAVE_PATH , SAVE_FOLDER_NAME)\n",
    "        if not os.path.exists(file_folder):\n",
    "            os.makedirs(file_folder)\n",
    "        file_name = wr.unique(image_series, tumor_index, user_id)\n",
    "        file_name = os.path.join(file_folder,file_name)\n",
    "        wr.write(weasis_raw_data, file_name)\n",
    "\n",
    "        Metrics_vol.append( [image_series_path , file_name , user_id ])\n",
    "\n",
    "\n",
    "\n",
    "    df_metrics = pd.DataFrame(Metrics_vol, \n",
    "                              columns = ['Image File Path','Contour File Path','Uni']) \n",
    "    df_metrics.to_csv('{}.csv'.format(SAVE_FOLDER_NAME) , index=False)\n",
    "    print('finished ', k )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls *.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_scalenas)",
   "language": "python",
   "name": "conda_scalenas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0b55b1b0709b9885fc17cbb53dd5636d97b629761f5c310468012a5343ed5bef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
