{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to load predicted mask for hard-disk and calculate Segmentation evaluation metics.\n",
    "predicted mask is the output of ScaleNASv2 Test and save predicted mask.ipynb\n",
    "\n",
    "cp -v /mnt/fast-data/mjc/AutoRECIST/Codes/ScaleNAS/ScaleNASv1/tools/utils_test.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "site_list = [8] \n",
    "cache_path = './cache/'\n",
    "name = 'PDS_AMGEN_20020408_22Cat_test'\n",
    "cache_filepath = os.path.join(cache_path, name+'_gt_roidb.pkl')\n",
    "\n",
    "\n",
    "sv_dir = mask_name = 'mask_1988'\n",
    "sv_path = os.path.join(sv_dir, 'test_val_results')\n",
    "\n",
    "DL_info = '/mnt/fast-data/mjc/AutoRECIST/Inputs/PDS_AMGEN_20020408_CIA-LAB_Image_And_Contour_2022-01-10.xlsx'\n",
    "metrics_save_path = 'temp_%s_.csv'%'ScaleNAS_Q5_9Slices_Amgen_Liver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# # This file is for segmetation metrics evaluation in 3D\n",
    "# Edited by Jingchen around 06/20/2021\n",
    "# This file is after ScaleNAS test which save predition into png images.\n",
    "# This file load png images as predicted contours in 2D\n",
    "# load cache pkl as gold-standard contours in 2D\n",
    "# The stack 2D based on dicom-header to get 3D\n",
    "# Evaluate 3D metics of dice, IoU, over-segmetation, and under-segmetation\n",
    "# \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import _init_paths\n",
    "\n",
    "from config import cfg\n",
    "from config import update_config\n",
    "from utils_test import *\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from utils.utils import get_confusion_matrix\n",
    "def convert_name(name):\n",
    "    new = name.replace('/','_')\n",
    "    return new\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached gt_roidb from %s ./cache/PDS_AMGEN_20020408_22Cat_test_gt_roidb.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/fast-data/mjc/envs/Lesions/lib/python3.6/site-packages/matplotlib/__init__.py:1405: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Loading cached gt_roidb from %s', cache_filepath)\n",
    "with open(cache_filepath, 'rb') as fp:\n",
    "    cached_roidb = pickle.load(fp)\n",
    "    \n",
    "roidb = cached_roidb\n",
    "\n",
    "from utils_test import __get_annotation__\n",
    "\n",
    "all_boxes = [ [ np.zeros((0,5),dtype=\"float32\") for _ in range(len(roidb)) ] for _ in range( cfg.DATASET.NUM_CLASSES) ]\n",
    "all_segms = [ [ [] for _ in range(len(roidb)) ] for _ in range( cfg.DATASET.NUM_CLASSES) ]\n",
    "\n",
    "for i in range(len(roidb)):\n",
    "    if i%500==0:\n",
    "        print('='*40,i)\n",
    "\n",
    "    one = roidb[i]\n",
    "    onename = one['image']\n",
    "    pred_im = Image.open(os.path.join(sv_path, convert_name(onename) ))\n",
    "    pred = np.array(pred_im)\n",
    "    for j in range(cfg.DATASET.NUM_CLASSES):\n",
    "        mask = np.asarray( pred==j , dtype=np.uint8)\n",
    "        if np.sum(mask > 0) <= 3 :\n",
    "            continue\n",
    "        segmentation, bbox, area = __get_annotation__(mask , xywh = False , bbox_score=True)\n",
    "        if segmentation and bbox:\n",
    "            all_segms[j][i] = segmentation\n",
    "            all_boxes[j][i] = bbox\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_CT = {}\n",
    "for i , aroidb in enumerate(roidb):\n",
    "    dicom_path , png_name = os.path.split(aroidb['image'])\n",
    "    slice_no , _= os.path.splitext(png_name)\n",
    "    slice_no = int(slice_no)\n",
    "    if slice_no != aroidb['slice_no']:\n",
    "        print(dicom_path,slice_no,aroidb['slice_no'])\n",
    "\n",
    "    segmentations = {}\n",
    "    bboxes = {}\n",
    "    for j in site_list:\n",
    "        segmentations[j] = all_segms[j][i]\n",
    "        bboxes[j] = all_boxes[j][i]\n",
    "\n",
    "    if dicom_path not in D_CT:\n",
    "        D_CT[dicom_path] = {}\n",
    "        D_CT[dicom_path][slice_no] = [aroidb , bboxes , segmentations]\n",
    "    else:\n",
    "        D_CT[dicom_path][slice_no] = [aroidb , bboxes , segmentations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "if DL_info[-4:]=='.csv':\n",
    "    df = pd.read_csv(DL_info)\n",
    "else:\n",
    "    df = pd.read_excel(DL_info)\n",
    "\n",
    "def pd_str_replace(df , col, ori, new):\n",
    "    if isinstance(col , str):\n",
    "        df[col] = df[col].str.replace(ori,new, case = False) \n",
    "    elif isinstance(col, list):\n",
    "        for one in col:\n",
    "            pd_str_replace(df , one, ori, new)\n",
    "    else:\n",
    "        raise('col instence should be str or list')\n",
    "\n",
    "pd_str_replace(df, ['Image File Path' , 'Contour File Path'], \"X:\" , \"/mnt/W-drive\")\n",
    "pd_str_replace(df, ['Image File Path' , 'Contour File Path'], r\"\\\\\" , \"/\")\n",
    "pd_str_replace(df, ['Image File Path'], \"/mnt/W-drive/ClinicalTrialDone/FNIH_VOLPACK\", \"/mnt/fast-disk1/mjc/AutoRecist/Inputs\")\n",
    "pd_str_replace(df, ['Image File Path'], \"/mnt/W-drive/ClinicalTrials\", \"/mnt/fast-disk1/mjc/AutoRecist/Inputs\")\n",
    "pd_str_replace(df, ['Contour File Path'], \"/mnt/W-drive/ConvWeasisToRaw\", \"/mnt/fast-disk1/mjc/AutoRecist/Inputs/ConvWeasisToRaw\")\n",
    "\n",
    "\n",
    "sys.path.append('/mnt/fast-data/mjc/DeepLesion/Codes/CollectData/')\n",
    "from print_weasis_raw_data import WeasisRawDataPrinter\n",
    "from read_weasis_raw_data import WeasisRawFileReader\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "D_dir2header_df = {}\n",
    "def get_dicom_header_df(image_dir , labels = []):\n",
    "    global D_dir2header_df\n",
    "    if image_dir in D_dir2header_df:\n",
    "        return D_dir2header_df[image_dir]\n",
    "\n",
    "    # image_dir = row['Image File Path']\n",
    "\n",
    "\n",
    "    labels = ['ImageName','InstanceNumber',\n",
    "            'BitsAllocated', 'BitsStored', 'Columns', 'HighBit', \n",
    "            'ImageOrientationPatient_0', 'ImageOrientationPatient_1', 'ImageOrientationPatient_2',\n",
    "            'ImageOrientationPatient_3', 'ImageOrientationPatient_4', 'ImageOrientationPatient_5',\n",
    "            'ImagePositionPatient_0', 'ImagePositionPatient_1', 'ImagePositionPatient_2',\n",
    "            'Modality', 'PatientID', 'PhotometricInterpretation', 'PixelRepresentation',\n",
    "            'PixelSpacing_0', 'PixelSpacing_1', 'RescaleIntercept', 'RescaleSlope', 'Rows', 'SOPInstanceUID',\n",
    "            'SamplesPerPixel', 'SeriesInstanceUID', 'StudyID', 'StudyInstanceUID', \n",
    "            'WindowCenter', 'WindowWidth', \n",
    "        ] if not labels else labels\n",
    "\n",
    "    data = {l: [] for l in labels}\n",
    "    \n",
    "    ctList = os.listdir(image_dir)\n",
    "    ctList.sort()\n",
    "\n",
    "    for image in ctList:\n",
    "        if '.dcm' not in image:\n",
    "            continue\n",
    "        if os.path.getsize(os.path.join(image_dir, image)) < 5*1024:\n",
    "            print('%s size < 5kb skiped!'%os.path.join(image_dir, image) )\n",
    "            continue\n",
    "        data[\"ImageName\"].append(image)\n",
    "\n",
    "        ds = pydicom.dcmread(os.path.join(image_dir, image))\n",
    "        for metadata in ds.dir():\n",
    "            if metadata not in data and metadata not in ['ImageOrientationPatient','ImagePositionPatient','PixelSpacing']:\n",
    "                continue\n",
    "            if metadata != \"PixelData\":\n",
    "                metadata_values = getattr(ds, metadata)\n",
    "                if type(metadata_values) == pydicom.multival.MultiValue and metadata not in [\"WindowCenter\", \"WindowWidth\"]:\n",
    "                    for i, v in enumerate(metadata_values):\n",
    "                        data[f\"{metadata}_{i}\"].append(v)  \n",
    "                else:\n",
    "\n",
    "                    if type(metadata_values) == pydicom.multival.MultiValue and metadata in [\"WindowCenter\", \"WindowWidth\"]:\n",
    "                        data[metadata].append(metadata_values[0])\n",
    "                    else:\n",
    "                        \n",
    "                        if metadata in ['ImageOrientationPatient','ImagePositionPatient','PixelSpacing']:\n",
    "                            print( 'error of loading key: {}'.format(metadata) )                    \n",
    "                        else:\n",
    "                            data[metadata].append(metadata_values)\n",
    "\n",
    "    df_image = pd.DataFrame(data).set_index(\"InstanceNumber\")\n",
    "    D_dir2header_df[image_dir] = df_image\n",
    "    return df_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_site(location , site_list):\n",
    "    for i in site_list:\n",
    "        if location == ix2labelname(i):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils_metrics_3d import *\n",
    "Metrics_vol = []\n",
    "Missed_vol = []\n",
    "previous_CT_ss = ''\n",
    "for _,row in df.iterrows():\n",
    "    if not compare_site(row.Location , site_list):\n",
    "#         only calculate one lesion site\n",
    "        continue\n",
    "\n",
    "\n",
    "    df_image = get_dicom_header_df( row['Image File Path'] )\n",
    "    instanceNumber_list = df_image.index.to_list()\n",
    "    D_z_index = instanceNumber2Matrix_z_index(instanceNumber_list)\n",
    "\n",
    "    reader = WeasisRawFileReader()\n",
    "    weasis_raw_data = reader.read_weasis_raw_file(row['Contour File Path'])\n",
    "\n",
    "    ss = row['Image File Path']\n",
    "    ss = ss.replace('/Inputs/' , '/Pngs/')\n",
    "    if ss not in D_CT:\n",
    "        print(ss , 'not in D_CT and skip it')\n",
    "        continue\n",
    "\n",
    "    mask_vol = initialize_mask_vol(weasis_raw_data , D_z_index)\n",
    "\n",
    "    slice_list = weasis_raw_data.get_instance_number_array()\n",
    "    for j, one in enumerate(slice_list):\n",
    "        file_name = InstanceNumber2file_name(df_image, one)\n",
    "        file_name = os.path.join( row['Image File Path'] , file_name)   \n",
    "        mask = weasis_raw_data.get_mask_image_2d(j)\n",
    "        mask_vol[D_z_index[one]] = mask\n",
    "\n",
    "    if ss != previous_CT_ss:\n",
    "        oneCT = D_CT[ss]\n",
    "        vol_gt , vol_pred = get_gt_and_pred_vols( oneCT, site_list, mask_vol.shape, D_z_index, union_mask=False )\n",
    "        previous_CT_ss = ss\n",
    "    hit = vols_seg_results(mask_vol , vol_pred, CTname=row['Contour File Path'], gt_keep_largest=1)\n",
    "    Metrics_vol.extend(hit)\n",
    "    \n",
    "    _n = len(Metrics_vol)\n",
    "    if _n%100==0 or _n in [1,3,5,10,30,50]:\n",
    "        print(_, _n)\n",
    "        df_metrics = pd.DataFrame(Metrics_vol, \n",
    "                                  columns = ['file_name','igt','merge','#gt','#pred',\n",
    "                                             'iou_score', 'dice_score', 'over_seg' , 'under_seg',\n",
    "                                             'area_gt','area_pred','intersection','union']) \n",
    "        df_metrics.to_csv(metrics_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metrics.to_csv(metrics_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(name)\n",
    "pd.options.display.float_format = \"{:.3f}\".format\n",
    "df_metrics = pd.DataFrame(Metrics_vol, \n",
    "                          columns = ['file_name','igt','merge','#gt','#pred',\n",
    "                                     'iou_score', 'dice_score', 'over_seg' , 'under_seg',\n",
    "                                     'area_gt','area_pred','intersection','union']) \n",
    "df_metrics.describe([.05, .25, .5, .75, .95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_metrics[df_metrics.dice_score>0.25].describe([.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfinnermerge = pd.merge(df_metrics,df,how='inner',left_on='file_name' , right_on='Contour File Path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in dfinnermerge.columns.tolist():\n",
    "    print(col , len(set(dfinnermerge[col].tolist() )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pts = dfinnermerge[\"Image File Path\"].values.tolist()\n",
    "FPs = []\n",
    "for onept in list(set(pts)):\n",
    "    df_onept = dfinnermerge[dfinnermerge[\"Image File Path\"]==onept]\n",
    "    assert( min(df_onept[\"#pred\"]) == max(df_onept[\"#pred\"]))\n",
    "    fp = max(df_onept[\"#pred\"])\n",
    "    FPs.append(fp)\n",
    "\n",
    "print( len(FPs) , sum(FPs) )\n",
    "\n",
    "dices = dfinnermerge.dice_score.tolist()\n",
    "\n",
    "for th in [0, 0.1, 0.2, 0.25, 0.5]:\n",
    "    TP = [p>th for p in dices ]\n",
    "    assert( len(TP) == len(dices))\n",
    "    fprate = ( sum(FPs) - sum(TP) ) / len(FPs)\n",
    "    print(f\"sensitivity is {sum(TP)/len(dices):.3f}({sum(TP)}/{len(dices)}) FP-rate is {fprate:.1f} at threshold {th}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum( dfinnermerge['#pred'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Cougar\n",
    "96 pts at baseline timepoints, 209 LN lesions\n",
    "1489 AI detections\n",
    "When Dice>0,25 means the lesion is detected\n",
    "Sensitivity 66%=138/209 \n",
    "FP rate is 13.2 FPs/CT (too high!)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = '/mnt/fast-disk1/mjc/AutoRecist/Outputs/ScaleNAS_Q5_9Slices/'\n",
    "name = 'Cougar_baseline'\n",
    "SHOW_LABEL = True\n",
    "SHOW_BOX = False\n",
    "SHOW_MASK = True\n",
    "SHOW_UNION_MASK= False\n",
    "SHOW_MASK_LABEL = True\n",
    "SHOW_GT_MASK = False\n",
    "\n",
    "def get_proper_CT_windowing(segmentations):\n",
    "\n",
    "    if 8 in segmentations and segmentations[8] :\n",
    "        return -160 , 240\n",
    "    if 7 in segmentations and segmentations[7] :\n",
    "        return -160 , 240    \n",
    "    if 9 in segmentations and segmentations[9] :\n",
    "        return -1250 , 250\n",
    "    return None\n",
    "\n",
    "keys = list(D_CT.keys())\n",
    "\n",
    "def compute_colors_for_labels(i):\n",
    "    D = {\n",
    "        1 : [1, 127, 31] , # abdomen\n",
    "        2 : [64, 255, 64] , #  abdomen LN\n",
    "        3 : [255, 64, 255] , # adrenal\n",
    "        4 : [64, 255, 64] , # axillary LN\n",
    "        5 : [5, 125, 155] , # bone\n",
    "        6 : [64, 255, 64] , #  inguinal LN\n",
    "        7 : [7, 124, 217] , # kidney\n",
    "        8 : [8, 251, 248] , # liver\n",
    "        9 : [255, 64, 64] , # lung\n",
    "        10 : [64, 255, 64] , # mediastinum LN\n",
    "        11 : [64, 255, 64] , # neck LN\n",
    "        12 : [12, 249, 117] , # ovary\n",
    "        13 : [13, 121, 148] , # pancreas\n",
    "        14 : [64, 255, 64] , # pelvic LN\n",
    "        15 : [15, 120, 210] , # pelvis\n",
    "        16 : [16, 247, 241] , # pleural\n",
    "        17 : [64, 255, 64] , #  retroperitoneal LN\n",
    "        18 : [18, 246, 48] , # soft tissue\n",
    "        19 : [19, 118, 79] , # spleen\n",
    "        20 : [20, 245, 110] , # stomach\n",
    "        21 : [21, 117, 141] , # thyroid\n",
    "        }\n",
    "    return D[i]\n",
    "\n",
    "for k in keys:\n",
    "    \n",
    "    oneCT = remove_single_slice_segms(D_CT[k])\n",
    "    savepath = os.path.join( SAVE_PATH , 'CTs_%s/%s'%(name,convert_name_compact(k)) )\n",
    "\n",
    "    print('Image results are saving into {}'.format(savepath))\n",
    "\n",
    "    if not os.path.exists(savepath):\n",
    "        os.makedirs(savepath)\n",
    "    \n",
    "    for s in oneCT:\n",
    "        aroidb , bboxes , segmentations = oneCT[s]\n",
    "\n",
    "        image_path = os.path.join(aroidb['image'])\n",
    "    \n",
    "        CT_windowing = get_proper_CT_windowing(segmentations)\n",
    "        if CT_windowing:\n",
    "            HU1, HU2 = CT_windowing\n",
    "        else:\n",
    "            [HU1, HU2 ] = aroidb['windows']\n",
    "            \n",
    "\n",
    "        image = load_image(image_path, HU1, HU2)\n",
    "        height,width = image.shape\n",
    "    #     plt.imshow(image)\n",
    "        image = np.dstack((image,image,image))\n",
    "        \n",
    "        if SHOW_MASK:\n",
    "            for j in site_list:\n",
    "                if j==0:\n",
    "                    continue\n",
    "                contours = segmentations[j]\n",
    "                colors = compute_colors_for_labels(j)\n",
    "                label = ix2labelname(j)\n",
    "                for c in contours:\n",
    "                    c = np.reshape(c,(-1,2))\n",
    "                    if c.shape[0]:\n",
    "                        image = cv2.drawContours(image, [np.int64( c  )], -1, colors, 1)\n",
    "                        if SHOW_MASK_LABEL:\n",
    "                            x,y,_,_ =ploy2boxes(c)\n",
    "                            template = \"{}\"\n",
    "                            if len(label)>=9:\n",
    "                                label=label[:3]+label[-3:]\n",
    "                \n",
    "                            s = template.format(label)\n",
    "                            cv2.putText(\n",
    "                                image, s, (x, y), cv2.FONT_HERSHEY_SIMPLEX, .5, colors, 1\n",
    "                            )  \n",
    "        \n",
    "\n",
    "        cv2.imwrite( os.path.join(savepath, convert_name_compact( aroidb['image'] ) ) , image )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_lesions)",
   "language": "python",
   "name": "conda_lesions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
